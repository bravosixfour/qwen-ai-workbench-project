# HPC Lab Deployment Guide
**Qwen Image Edit AI Workbench - Custom Lab Infrastructure**

## üèóÔ∏è Your Infrastructure Analysis

### **HPC-1: The Powerhouse** üöÄ
- **Capability**: 192GB VRAM (2x RTX PRO 6000 Blackwell)
- **Role**: Primary production inference server
- **Strength**: Can handle the largest models and biggest batches
- **Best For**: Complex multi-image editing, large-scale processing

### **HPC-2: The Parallel Beast** ‚ö°
- **Capability**: 96GB VRAM (3x RTX 5090) 
- **Role**: Multi-user, distributed processing
- **Strength**: Excellent for parallel workloads
- **Best For**: Multiple simultaneous users, load balancing

### **HPC-3: The Dev Workhorse** üîß
- **Capability**: 48GB VRAM (L40)
- **Role**: Development and testing environment
- **Strength**: Perfect for experimentation and model testing
- **Best For**: Development, single-user workflows

## üéØ Recommended Deployment Strategy

### **Phase 1: Start with HPC-1** 
```bash
# HPC-1 Priority Deployment
System: HPC-1 (192GB VRAM)
Config: Maximum performance, full model loading
Expected: 40-60 images/minute, 16-24 concurrent requests
```

**Why HPC-1 First:**
- **Massive VRAM**: 192GB can handle any model size
- **Blackwell Architecture**: Latest GPU technology
- **Watercooling**: Sustained performance under load
- **Future-Proof**: Ready for even larger models

### **Phase 2: Add HPC-2 for Scale**
```bash
# HPC-2 Multi-GPU Setup  
System: HPC-2 (3x RTX 5090)
Config: Pipeline parallel processing
Expected: 25-35 images/minute, 12-18 concurrent requests
```

### **Phase 3: HPC-3 for Development**
```bash
# HPC-3 Development Environment
System: HPC-3 (L40 48GB)
Config: Single GPU, quantized models
Expected: 15-25 images/minute, 6-8 concurrent requests
```

## üîß Proxmox VM Configuration

### **Optimal VM Setup:**

#### **HPC-1 VM:**
```yaml
CPU: 24 cores (leave 8 for host)
RAM: 200GB (leave 56GB for host)  
GPU: Both RTX PRO 6000 (passthrough)
Storage: 2TB NVMe (models + cache)
Network: 10GbE bridge
```

#### **HPC-2 VM:**
```yaml
CPU: 24 cores
RAM: 200GB  
GPU: All 3x RTX 5090 (passthrough)
Storage: 2TB NVMe
Network: 10GbE bridge
```

#### **HPC-3 VM:**
```yaml
CPU: 20 cores (leave 4 for host)
RAM: 96GB (leave 32GB for host)
GPU: L40 (passthrough)
Storage: 1TB NVMe
Network: 10GbE bridge
```

## üåä Watercooling Advantages

Your centralized watercooling provides **massive advantages** for AI workloads:

1. **Sustained Performance**: No thermal throttling during long inference runs
2. **Consistent Timing**: Stable temperatures = predictable inference times  
3. **Higher Utilization**: Can push GPUs harder for longer periods
4. **Quiet Operation**: Better lab environment for development work

## üöÄ Deployment Commands

### **1. Deploy on HPC-1 (Primary):**
```bash
# Copy project to HPC-1
scp -r qwen-ai-workbench-project/ hpc-1:/data/

# Deploy with HPC-1 optimizations
cd /data/qwen-ai-workbench-project
python code/scripts/deploy.py --env hpc-1

# Access at: http://hpc-1:8001
```

### **2. Deploy on HPC-2 (Multi-GPU):**
```bash
# Deploy with 3-GPU configuration
python code/scripts/deploy.py --env hpc-2

# Access at: http://hpc-2:8002
```

### **3. Deploy on HPC-3 (Development):**
```bash
# Deploy with single GPU configuration
python code/scripts/deploy.py --env hpc-3

# Access at: http://hpc-3:8003
```

## üìä Expected Performance

| System | Images/Min | Concurrent Users | Best Use Case |
|--------|------------|------------------|---------------|
| **HPC-1** | 40-60 | 16-24 | Production inference |
| **HPC-2** | 25-35 | 12-18 | Multi-user environment |
| **HPC-3** | 15-25 | 6-8 | Development/testing |

## üîó Load Balancing Setup

### **Nginx Configuration:**
```nginx
upstream qwen_cluster {
    server hpc-1.lab:8001 weight=3;  # Highest capacity
    server hpc-2.lab:8002 weight=2;  # Good capacity  
    server hpc-3.lab:8003 weight=1;  # Backup/dev
}

server {
    listen 8000;
    location / {
        proxy_pass http://qwen_cluster;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## üí° Pro Tips for Your Setup

### **HPC-1 Optimization:**
- Enable model sharding across both RTX PRO 6000
- Use `bf16` precision (optimal for Blackwell)
- Set batch size to 16+ (you have the VRAM!)

### **HPC-2 Multi-GPU Strategy:**
- Pipeline parallel: Each GPU handles different model stages
- Perfect for multiple simultaneous inference requests
- Great for A/B testing different models

### **HPC-3 Development:**
- Use quantized models (8-bit/4-bit) for faster iteration
- Perfect for testing new configurations before deploying to production
- Ideal for fine-tuning experiments

## üéÆ Quick Start Commands

```bash
# 1. Start HPC-1 (primary)
ssh hpc-1
cd /data/qwen-ai-workbench-project
./start-primary.sh

# 2. Start HPC-2 (secondary)  
ssh hpc-2
cd /data/qwen-ai-workbench-project
./start-secondary.sh

# 3. Access unified interface
open http://hpc-1:3001  # Mango Editor UI
open http://hpc-1:8001/docs  # API Documentation
```

Your lab setup is **absolutely perfect** for this deployment - the combination of massive VRAM, watercooling, and 10GbE networking will deliver exceptional performance! üî•